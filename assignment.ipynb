{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55fa0ac4",
   "metadata": {},
   "source": [
    "# Assignment 6: Neural Network Showdown\n",
    "\n",
    "Build and compare neural network architectures on image and time-series data.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebeae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt\n",
    "\n",
    "# GPU acceleration (platform-specific)\n",
    "import platform\n",
    "if platform.system() == \"Darwin\" and platform.machine() == \"arm64\":\n",
    "    %pip install -q tensorflow-metal\n",
    "\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Report available accelerators\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPU acceleration: {len(gpus)} device(s)\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  {gpu.name}\")\n",
    "else:\n",
    "    print(\"No GPU detected — using CPU\")\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import (\n",
    "    Dense, Flatten, Dropout, Conv2D, MaxPooling2D, LSTM, Input\n",
    ")\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from helpers import (\n",
    "    load_cifar10, load_ecg5000,\n",
    "    plot_training_history, plot_confusion_matrix,\n",
    "    plot_sample_images, plot_ecg_traces, plot_predictions,\n",
    "    CIFAR10_CLASSES, ECG_CLASSES,\n",
    ")\n",
    "\n",
    "OUTPUT_DIR = \"output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f79d00",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Dense Baseline on CIFAR-10\n",
    "\n",
    "**Task:** Build a Dense (fully connected) network to classify CIFAR-10 images.\n",
    "\n",
    "CIFAR-10 has 60,000 color images (32x32x3) across 10 classes. A Dense network\n",
    "flattens each image into 3,072 numbers and classifies from there. This is our\n",
    "baseline — it ignores spatial structure entirely.\n",
    "\n",
    "**Architecture requirements:**\n",
    "- Flatten the input\n",
    "- At least 2 hidden Dense layers with ReLU activation\n",
    "- Dropout after each hidden layer\n",
    "- Output: Dense(10, softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e592393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Part 1: Dense Baseline on CIFAR-10\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Load data (normalized to [0,1], one-hot encoded)\n",
    "X_train, y_train, X_test, y_test = load_cifar10()\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training images to verify data loaded correctly\n",
    "plot_sample_images(X_train, y_train, CIFAR10_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc56b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a Dense model using Sequential\n",
    "# Tip: call model_dense.summary() after building to verify your architecture\n",
    "# Requirements:\n",
    "#   - Input(shape=(32, 32, 3))\n",
    "#   - Flatten()\n",
    "#   - At least 2 Dense hidden layers with activation='relu'\n",
    "#   - Dropout after each hidden Dense layer\n",
    "#   - Dense(10, activation='softmax') as output\n",
    "model_dense = None  # replace with your Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52067ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compile the model\n",
    "# model_dense.compile(optimizer='adam',\n",
    "#                     loss='categorical_crossentropy',\n",
    "#                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333c3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train with EarlyStopping\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=3,\n",
    "#                            restore_best_weights=True)\n",
    "# history_dense = model_dense.fit(X_train, y_train,\n",
    "#                                 epochs=20, batch_size=128,\n",
    "#                                 validation_split=0.1,\n",
    "#                                 callbacks=[early_stop])\n",
    "history_dense = None  # replace with your fit call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate on test set\n",
    "# test_loss, test_acc = model_dense.evaluate(X_test, y_test, verbose=0)\n",
    "test_acc = None  # replace\n",
    "\n",
    "# TODO: Generate predictions and confusion matrix\n",
    "# y_pred = np.argmax(model_dense.predict(X_test, verbose=0), axis=1)\n",
    "# y_true = np.argmax(y_test, axis=1)\n",
    "# cm = confusion_matrix(y_true, y_pred)\n",
    "y_pred = None  # replace\n",
    "y_true = None  # replace\n",
    "cm = None  # replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6de654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: visualize predictions and confusion matrix to diagnose issues\n",
    "# plot_predictions(X_test, y_true, y_pred, CIFAR10_CLASSES)\n",
    "# plot_confusion_matrix(y_true, y_pred, list(CIFAR10_CLASSES.values()),\n",
    "#                       os.path.join(OUTPUT_DIR, \"part1_confusion_matrix.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b65a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results (do not modify this cell)\n",
    "results = {\n",
    "    \"accuracy\": float(test_acc),\n",
    "    \"confusion_matrix\": cm.tolist(),\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"part1_results.json\"), \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Dense accuracy: {test_acc:.4f}\")\n",
    "print(\"Saved output/part1_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea4daa2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: CNN on CIFAR-10\n",
    "\n",
    "**Task:** Build a CNN to classify the same CIFAR-10 images. Compare its accuracy\n",
    "to the Dense baseline from Part 1.\n",
    "\n",
    "CNNs use convolutional filters that slide across the image, detecting local\n",
    "patterns (edges, textures, shapes). This preserves spatial structure that\n",
    "Dense layers discard.\n",
    "\n",
    "**Architecture requirements:**\n",
    "- At least 2 Conv2D + MaxPooling2D blocks\n",
    "- Flatten, then Dense hidden layer with Dropout\n",
    "- Output: Dense(10, softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a124f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPart 2: CNN on CIFAR-10\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Data is already loaded from Part 1 (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a CNN model using Sequential\n",
    "# Requirements:\n",
    "#   - Input(shape=(32, 32, 3))\n",
    "#   - At least 2 blocks of: Conv2D(filters, (3,3), activation='relu')\n",
    "#                            + MaxPooling2D((2,2))\n",
    "#   - Flatten()\n",
    "#   - Dense hidden layer with ReLU + Dropout\n",
    "#   - Dense(10, activation='softmax') as output\n",
    "# Tip: call model_cnn.summary() after building to check layer shapes and param counts\n",
    "model_cnn = None  # replace with your Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c9b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compile the model\n",
    "# model_cnn.compile(optimizer='adam',\n",
    "#                   loss='categorical_crossentropy',\n",
    "#                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train with EarlyStopping and ModelCheckpoint\n",
    "# callbacks = [\n",
    "#     EarlyStopping(monitor='val_loss', patience=3,\n",
    "#                   restore_best_weights=True),\n",
    "#     ModelCheckpoint('output/best_cnn.keras',\n",
    "#                     save_best_only=True, monitor='val_accuracy'),\n",
    "# ]\n",
    "# history_cnn = model_cnn.fit(X_train, y_train,\n",
    "#                             epochs=15, batch_size=64,\n",
    "#                             validation_split=0.1,\n",
    "#                             callbacks=callbacks)\n",
    "history_cnn = None  # replace with your fit call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd07834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot training history\n",
    "# plot_training_history(history_cnn,\n",
    "#                       os.path.join(OUTPUT_DIR, \"part2_training_history.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb0b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate on test set\n",
    "# cnn_loss, cnn_acc = model_cnn.evaluate(X_test, y_test, verbose=0)\n",
    "cnn_acc = None  # replace\n",
    "\n",
    "# TODO: Generate predictions and confusion matrix\n",
    "# y_pred_cnn = np.argmax(model_cnn.predict(X_test, verbose=0), axis=1)\n",
    "# y_true_cnn = np.argmax(y_test, axis=1)\n",
    "# cm_cnn = confusion_matrix(y_true_cnn, y_pred_cnn)\n",
    "y_pred_cnn = None  # replace\n",
    "y_true_cnn = None  # replace\n",
    "cm_cnn = None  # replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d455628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: visualize predictions and confusion matrix to diagnose issues\n",
    "# plot_predictions(X_test, y_true_cnn, y_pred_cnn, CIFAR10_CLASSES)\n",
    "# plot_confusion_matrix(y_true_cnn, y_pred_cnn, list(CIFAR10_CLASSES.values()),\n",
    "#                       os.path.join(OUTPUT_DIR, \"part2_confusion_matrix.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results (do not modify this cell)\n",
    "results_cnn = {\n",
    "    \"accuracy\": float(cnn_acc),\n",
    "    \"confusion_matrix\": cm_cnn.tolist(),\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"part2_results.json\"), \"w\") as f:\n",
    "    json.dump(results_cnn, f, indent=2)\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\"model\": \"Dense\", \"accuracy\": float(test_acc)},\n",
    "    {\"model\": \"CNN\", \"accuracy\": float(cnn_acc)},\n",
    "])\n",
    "comparison.to_csv(os.path.join(OUTPUT_DIR, \"part2_comparison.csv\"), index=False)\n",
    "\n",
    "print(f\"CNN accuracy:   {cnn_acc:.4f}\")\n",
    "print(f\"Dense accuracy: {test_acc:.4f}\")\n",
    "print(f\"Improvement:    {cnn_acc - test_acc:+.4f}\")\n",
    "print(\"Saved output/part2_results.json and output/part2_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f949ce7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: LSTM on ECG5000\n",
    "\n",
    "**Task:** Build an LSTM to classify heartbeat recordings.\n",
    "\n",
    "ECG5000 contains 5,000 heartbeat recordings — each is 140 time steps of voltage\n",
    "measurements, classified into 5 types (Normal, Supraventricular, Premature\n",
    "Ventricular, Fusion, Unknown). This is sequential data where order matters,\n",
    "making it a natural fit for recurrent networks.\n",
    "\n",
    "**Architecture requirements:**\n",
    "- LSTM layer (any reasonable number of units)\n",
    "- Dropout for regularization\n",
    "- Dense output with softmax (5 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPart 3: LSTM on ECG5000\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Load ECG data (already shaped for RNN input)\n",
    "X_train_ecg, y_train_ecg, X_test_ecg, y_test_ecg = load_ecg5000()\n",
    "print(f\"Train: {X_train_ecg.shape}, Test: {X_test_ecg.shape}\")\n",
    "print(f\"Classes: {list(ECG_CLASSES.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ECG traces to understand the data\n",
    "plot_ecg_traces(X_train_ecg, y_train_ecg, ECG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789dec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build an LSTM model using Sequential\n",
    "# Requirements:\n",
    "#   - Input(shape=(140, 1))\n",
    "#   - LSTM layer (e.g., 64 units)\n",
    "#   - Dropout\n",
    "#   - Dense(5, activation='softmax')\n",
    "# Tip: call model_lstm.summary() after building to verify your architecture\n",
    "model_lstm = None  # replace with your Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0f4a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compile the model\n",
    "# model_lstm.compile(optimizer='adam',\n",
    "#                    loss='categorical_crossentropy',\n",
    "#                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a2b744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train with EarlyStopping\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=5,\n",
    "#                            restore_best_weights=True)\n",
    "# history_lstm = model_lstm.fit(X_train_ecg, y_train_ecg,\n",
    "#                               epochs=30, batch_size=32,\n",
    "#                               validation_split=0.1,\n",
    "#                               callbacks=[early_stop])\n",
    "history_lstm = None  # replace with your fit call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a9d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot training history\n",
    "# plot_training_history(history_lstm,\n",
    "#                       os.path.join(OUTPUT_DIR, \"part3_training_history.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b28180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate on test set\n",
    "# lstm_loss, lstm_acc = model_lstm.evaluate(X_test_ecg, y_test_ecg, verbose=0)\n",
    "lstm_acc = None  # replace\n",
    "\n",
    "# TODO: Generate predictions and confusion matrix\n",
    "# y_pred_ecg = np.argmax(model_lstm.predict(X_test_ecg, verbose=0), axis=1)\n",
    "# y_true_ecg = np.argmax(y_test_ecg, axis=1)\n",
    "# cm_ecg = confusion_matrix(y_true_ecg, y_pred_ecg)\n",
    "y_pred_ecg = None  # replace\n",
    "y_true_ecg = None  # replace\n",
    "cm_ecg = None  # replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a5a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: visualize confusion matrix to see which heartbeat types are confused\n",
    "# plot_confusion_matrix(y_true_ecg, y_pred_ecg, list(ECG_CLASSES.values()),\n",
    "#                       os.path.join(OUTPUT_DIR, \"part3_confusion_matrix.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results (do not modify this cell)\n",
    "results_ecg = {\n",
    "    \"accuracy\": float(lstm_acc),\n",
    "    \"confusion_matrix\": cm_ecg.tolist(),\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"part3_results.json\"), \"w\") as f:\n",
    "    json.dump(results_ecg, f, indent=2)\n",
    "\n",
    "print(f\"LSTM accuracy: {lstm_acc:.4f}\")\n",
    "print(\"Saved output/part3_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69849039",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00802a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAll parts complete!\")\n",
    "print(\"Run 'pytest .github/tests/ -v' in your terminal to check your work.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
